{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLtswBh-mJDQ"
   },
   "source": [
    "## 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TY6WT3HRmJEs"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# Install Prerequisites\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-learn scikit-surprise\n",
    "# !pip install git+https://github.com/gbolmier/funk-svd\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Preprocessing\n",
    "import random\n",
    "from time import time\n",
    "import cufflinks as cf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Models\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import SVD, NormalPredictor, BaselineOnly, NMF, SlopeOne, CoClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Performance Evaluation\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "\n",
    "# Display\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"white\")\n",
    "pd.set_option('display.max_columns', 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import comet_ml at the top of your file\n",
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment with your api key:\n",
    "experiment = Experiment(\n",
    "    api_key=\"GS3AtcawU4R2B2a6cEb67GGZi\",\n",
    "    project_name=\"edsa-recommender\",\n",
    "    workspace=\"janleg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j78j8qeKmJFI"
   },
   "source": [
    "## 2. loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "7JonSgz3mJFN",
    "outputId": "204f9aea-451b-40d0-cfb8-1e65f317f2ee"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2d8e35f9dd8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# laoding scores dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenom_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genome_scores.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgenom_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'genome_scores.csv'"
     ]
    }
   ],
   "source": [
    "# laoding scores dataset\n",
    "genom_score = pd.read_csv(\"genome_scores.csv\")\n",
    "genom_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MixozOkbmJFa"
   },
   "outputs": [],
   "source": [
    "# loading tags dataset\n",
    "genom_tags = pd.read_csv(\"genome_tags.csv\")\n",
    "genom_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0heNTFNmJFf"
   },
   "outputs": [],
   "source": [
    "# loading imdb_data dataset\n",
    "imdb = pd.read_csv(\"imdb_data.csv\")\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MGwpLximJFp"
   },
   "outputs": [],
   "source": [
    "# loading links dataset\n",
    "links = pd.read_csv(\"links.csv\")\n",
    "links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7u9EJ0kmJF2"
   },
   "outputs": [],
   "source": [
    "# loading movies dataset\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byDYZqlymJF_"
   },
   "outputs": [],
   "source": [
    "# loading tags dataset\n",
    "tags = pd.read_csv(\"tags.csv\")\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MolARkCnmJGN"
   },
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFwuZ9WBmJGT"
   },
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziguFMkGmJGY"
   },
   "outputs": [],
   "source": [
    "train_df = train.drop(\"timestamp\", 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSVI4XaRmJGg"
   },
   "outputs": [],
   "source": [
    "train_main = pd.merge(train_df, movies)\n",
    "train_main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLqXblWDmJGr"
   },
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUYVJqLKmJH8"
   },
   "outputs": [],
   "source": [
    "# average rating for each movie\n",
    "train_main.groupby('title')['rating'].mean().sort_values(ascending= False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTvR2OWgmJIs"
   },
   "outputs": [],
   "source": [
    "# Total number of rating per movie\n",
    "train_main.groupby('title')['rating'].count().sort_values(ascending= False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRHUb-9amJIv"
   },
   "outputs": [],
   "source": [
    "ratings = pd.DataFrame(train_main.groupby('title')['rating'].mean())\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6idSPzjmJIz"
   },
   "outputs": [],
   "source": [
    "ratings['num_of_ratings'] = pd.DataFrame(train_main.groupby('title')['rating'].count())\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwiRy05wmJI2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ratings['num_of_ratings'].hist(bins=70) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHF-FtakmJI6"
   },
   "outputs": [],
   "source": [
    "# distribution of ratings\n",
    "plt.figure(figsize=(20,8))\n",
    "ratings['rating'].hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwJKG_MxmJI_"
   },
   "outputs": [],
   "source": [
    "# distribution on=f the ratings\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    g = sns.factorplot(\"rating\", data=train_main, aspect=2.0, kind='count')\n",
    "    g.set_ylabels(\"Total number of ratings\")\n",
    "print(f'Average rating in dataset : {np.mean(train_main[\"rating\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQUQJzHRmJJO"
   },
   "outputs": [],
   "source": [
    "chunk_size = 5000\n",
    "chunks = [x for x in range(0, train_main.shape[0], chunk_size)]\n",
    "\n",
    "for i in range(0, len(chunks) -  1):\n",
    "    print(chunks[i], chunks[i + 1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkD5KvAmmJJS"
   },
   "outputs": [],
   "source": [
    "# distribution of movie genre\n",
    "plt.figure(figsize=(20,7))\n",
    "generlist = movies['genres'].apply(lambda generlist_movie : str(generlist_movie).split(\"|\"))\n",
    "geners_count = {}\n",
    "\n",
    "for generlist_movie in generlist:\n",
    "    for gener in generlist_movie:\n",
    "        if (geners_count.get(gener,False)):\n",
    "            geners_count[gener] = geners_count[gener]+1\n",
    "        else:\n",
    "            geners_count[gener] = 1\n",
    "#geners_count.pop(\"(No genre listed)\")\n",
    "plt.bar(geners_count.keys(), geners_count.values(), color='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATHhBXM4mJJW"
   },
   "outputs": [],
   "source": [
    "# grouping by rating based on users\n",
    "ratings_grouped_by_users = train_main.groupby('userId').agg([np.size, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUyI7Ga5mJJY"
   },
   "outputs": [],
   "source": [
    "ratings_grouped_by_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnLGZzRumJJa"
   },
   "outputs": [],
   "source": [
    "# top ten users who have rated most movies\n",
    "ratings_grouped_by_users['rating']['size'].sort_values(ascending=False).head(10).plot(kind = 'bar', figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_DzpNh-mJJd"
   },
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies = train_main.groupby('movieId').agg([np.mean], np.size)\n",
    "ratings_grouped_by_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XDbn0ramJJh"
   },
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies = ratings_grouped_by_movies.drop('userId', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTXyn5SVmJJi"
   },
   "outputs": [],
   "source": [
    "ratings_grouped_by_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X0CYYm9mJJk"
   },
   "outputs": [],
   "source": [
    "#movies with high average ratings\n",
    "ratings_grouped_by_movies['rating']['mean'].sort_values(ascending=False).head(20).plot(kind='bar', figsize=(7,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-quEo9_mJJl"
   },
   "outputs": [],
   "source": [
    "#movies with low average ratings \n",
    "low_rated_movies_filter = ratings_grouped_by_movies['rating']['mean']< 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voDLO1I7mJJm"
   },
   "outputs": [],
   "source": [
    "low_rated_movies = ratings_grouped_by_movies[low_rated_movies_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAQYnOWBmJJn"
   },
   "outputs": [],
   "source": [
    "low_rated_movies.head(20).plot(kind='bar', figsize=(7,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5nZOb8MmJJo"
   },
   "outputs": [],
   "source": [
    "low_rated_movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Hbbly51mJJq"
   },
   "outputs": [],
   "source": [
    "agg_ratings = train_main.groupby('title').agg(mean_rating = ('rating', 'mean'), \n",
    "                                      number_of_ratings = ('rating', 'count')).reset_index('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfChokyymJJr"
   },
   "outputs": [],
   "source": [
    "agg_ratings_5000 = agg_ratings[agg_ratings['number_of_ratings']>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ralVxCXmmJJs"
   },
   "outputs": [],
   "source": [
    "agg_ratings_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLw-Nw5ymJJu"
   },
   "outputs": [],
   "source": [
    "#checking popular movies\n",
    "agg_ratings_5000.sort_values(by = 'number_of_ratings', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nB6KWTeWK-Vp"
   },
   "outputs": [],
   "source": [
    "# creates a new column for publish year\n",
    "# shows the number of observations without publish year\n",
    "years = []\n",
    "\n",
    "for title in train_main['title']:\n",
    "    year_subset = title[-5:-1]\n",
    "    try: years.append(int(year_subset))\n",
    "    except: years.append(0)\n",
    "        \n",
    "train_main['moviePubYear'] = years\n",
    "print(len(train_main[train_main['moviePubYear'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX45EoniLXF5"
   },
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXjhjGm_AWR0"
   },
   "outputs": [],
   "source": [
    "print(\"Train: \")\n",
    "print(str(train.info()))\n",
    "print(\"============\")\n",
    "print(\"Test: \")\n",
    "print(str(test.info()))\n",
    "print(\"============\")\n",
    "print(\"Movies: \")\n",
    "print(str(movies.info()))\n",
    "# print(\"============\")\n",
    "# print(\"Tags: \")\n",
    "# print(str(tags.info()))\n",
    "print(\"============\")\n",
    "print(\"Links: \")\n",
    "print(str(links.info()))\n",
    "print(\"============\")\n",
    "print(\"IMDB: \")\n",
    "print(str(imdb.info()))\n",
    "print(\"============\")\n",
    "print(\"Genome score: \")\n",
    "print(str(genom_score.info()))\n",
    "print(\"============\")\n",
    "print(\"Genome tags: \")\n",
    "print(str(genom_tags.info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUe1aNRsAZE2"
   },
   "outputs": [],
   "source": [
    "print(\"Train: \")\n",
    "print(str(train.isnull().sum()))\n",
    "print(\"============\")\n",
    "print(\"Test: \")\n",
    "print(str(test.isnull().sum()))\n",
    "print(\"============\")\n",
    "print(\"Movies: \")\n",
    "print(str(movies.isnull().sum()))\n",
    "# print(\"============\")\n",
    "# print(\"Tags: \")\n",
    "# print(str(tags.isnull().sum()))\n",
    "print(\"============\")\n",
    "print(\"Links: \")\n",
    "print(str(links.isnull().sum()))\n",
    "print(\"============\")\n",
    "print(\"IMDB: \")\n",
    "print(str(imdb.isnull().sum()))\n",
    "print(\"============\")\n",
    "print(\"Genome score: \")\n",
    "print(str(genom_score.isnull().sum()))\n",
    "print(\"============\")\n",
    "print(\"Genome tags: \")\n",
    "print(str(genom_tags.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qr9Kb7ogAspm"
   },
   "outputs": [],
   "source": [
    "# Drop missing rows\n",
    "links.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvkVMVIlBHDL"
   },
   "source": [
    "### Scaling Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZIy-CaGAxcM"
   },
   "outputs": [],
   "source": [
    "# This might not be necessary as scores isbetween 0 and 1 already\n",
    "scaler_mds = StandardScaler()\n",
    "mds_genome = scaler_mds.fit_transform(genom_score.sample(frac=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lb0rogdMB8Km"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(3, n_jobs = -1, verbose = 2, perplexity = 10, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sbeMPIdCCpA"
   },
   "outputs": [],
   "source": [
    "tsne.fit(mds_genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYXDtkfxCMHo"
   },
   "outputs": [],
   "source": [
    "Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Add 3D scatter plot\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(tsne.embedding_[:,0], tsne.embedding_[:,1], tsne.embedding_[:,2], color='#4D17A0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYeluv00CScW"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x = tsne.embedding_[:,0], y = tsne.embedding_[:,1], size=tsne.embedding_[:,2],color='#4DA017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN4F-rRqClU8"
   },
   "outputs": [],
   "source": [
    "# Manually pivot table as data is too large for in-built functions\n",
    "def pivot_(df):\n",
    "    \"\"\"\n",
    "    Pivots table.\n",
    "    \"\"\"\n",
    "    new_dict = {'movieId':sorted(set(df.index))}\n",
    "    pivoted = pd.DataFrame(new_dict)\n",
    "    tagids = sorted(set(df['tagId']))\n",
    "    for Id in range(len(tagids)):\n",
    "        pivoted[f'{Id+1}'] = list(df[df['tagId'] == Id+1]['relevance'])\n",
    "    return pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAGYePSlCuJn"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted = pivot_(genom_score).set_index('movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy4x0CxLDAMm"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PRrxyBLDE1F"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.columns = list(genom_tags['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyvrgvnsDIRl"
   },
   "outputs": [],
   "source": [
    "pca_data_pivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLk-XzGkDUm9"
   },
   "outputs": [],
   "source": [
    "features = [col for col in pca_data_pivoted.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkxrpOwPDYsl"
   },
   "outputs": [],
   "source": [
    "# boxplot of unscaled features\n",
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "columns = random.sample(range(0, 1129), 20)\n",
    "pca_data_pivoted.iloc[:,columns].iplot(kind='box', title=\"Boxplots of Features (Unscaled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymozQ0vkDdSk"
   },
   "outputs": [],
   "source": [
    "# define a scaling function\n",
    "def scaler(df):\n",
    "    \"\"\"\n",
    "    Scales data.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler(with_std=True)\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S1RdmgjDjhs"
   },
   "outputs": [],
   "source": [
    "# apply function on data\n",
    "pca_scaled = scaler(pca_data_pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO0_J30DDqtt"
   },
   "outputs": [],
   "source": [
    "# convert into a data frame\n",
    "scaled_pca = pd.DataFrame(pca_scaled, index = pca_data_pivoted.index, columns = pca_data_pivoted.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7irM5trpD2HD"
   },
   "outputs": [],
   "source": [
    "# boxplot of scaled features\n",
    "cf.set_config_file(offline=True, world_readable=True, theme='white')\n",
    "# using plotly to plot the boxplot\n",
    "scaled_pca.iloc[:,columns].iplot(kind='box', title=\"Boxplots of Features (Scaled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TI2Q36TFD_zq"
   },
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyl7iPZYD7G7"
   },
   "outputs": [],
   "source": [
    "# define PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# fit the PCA model to our data and apply the dimensionality reduction \n",
    "prin_comp = pca.fit_transform(pca_data_pivoted[features])\n",
    "\n",
    "# create a dataframe containing the principal components\n",
    "pca_df = pd.DataFrame(data = prin_comp,\n",
    "                      index=pca_data_pivoted.index, columns=pca_data_pivoted.columns\n",
    "                     )\n",
    "\n",
    "# plot line graph of cumulative variance explained\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_),color='#4D17A0')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVG0qZ-rEKNq"
   },
   "outputs": [],
   "source": [
    "# use 85% of features\n",
    "pca_85 = PCA(.85)\n",
    "pca_85_df = pca_85.fit_transform(pca_data_pivoted)\n",
    "print(round(pca_85.explained_variance_ratio_.sum()*100, 1),\n",
    "      \"% of variance explained by\",\n",
    "      pca_85.n_components_,\n",
    "      \"components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVFoCPsSEPPM"
   },
   "outputs": [],
   "source": [
    "# create a data frame of features\n",
    "pca_85_df = pd.DataFrame(pca_85_df, index = pca_data_pivoted.index)\n",
    "pca_85_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNcUtGEWEwg_"
   },
   "source": [
    "### WCSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mFtfoZKETah"
   },
   "outputs": [],
   "source": [
    "# Manually implement the WCSS\n",
    "def within_cluster_variation(df, label_col='cluster_label'):\n",
    "    \"\"\"\n",
    "    Manually implements the WCSS.\n",
    "    \"\"\"\n",
    "    centroids = df.groupby(label_col).mean()\n",
    "    out = 0\n",
    "    for label, point in centroids.iterrows():\n",
    "        df_features = df[df[label_col] == label].drop(label_col, axis=1)\n",
    "        out += (df_features - point).pow(2).sum(axis=1).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7s5yOmTxEXGx"
   },
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 18 clusters where 18 is the number of genres\n",
    "n_clusters = np.arange(2, 19)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 19\n",
    "for k in n_clusters:\n",
    "    print(f'training model with {k} clusters')\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(pca_85_df)\n",
    "    \n",
    "    # measure BCSS\n",
    "    print(f'evaluating model with {k} clusters')\n",
    "    y_preds = km.predict(pca_85_df)\n",
    "    pca_85_df = pd.DataFrame(pca_85_df)\n",
    "    pca_85_df['cluster_label'] = y_preds\n",
    "    errors.append(within_cluster_variation(pca_85_df, 'cluster_label'))\n",
    "    print(errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHgsJJSpEcSK"
   },
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 18 clusters where 18 is the number of genres\n",
    "n_clusters = np.arange(2, 19)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 19\n",
    "for k in n_clusters:\n",
    "    print(f'training model with {k} clusters')\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(pca_85_df)\n",
    "    \n",
    "    # measure BCSS\n",
    "    print(f'evaluating model with {k} clusters')\n",
    "    y_preds = km.predict(pca_85_df)\n",
    "    pca_85_df = pd.DataFrame(pca_85_df)\n",
    "    pca_85_df['cluster_label'] = y_preds\n",
    "    errors.append(within_cluster_variation(pca_85_df, 'cluster_label'))\n",
    "    print(errors[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56f_k_vIE-Dj"
   },
   "source": [
    "### BCSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsKdrYT2Eo_B"
   },
   "outputs": [],
   "source": [
    "# Between cluster variation\n",
    "def between_cluster_variation(df, label_col='label'):\n",
    "    centroids = df.groupby(label_col).mean()\n",
    "    global_mean = df.drop(label_col, axis=1).mean()\n",
    "    centroid_count = df.groupby(label_col).size()\n",
    "    centroid_to_mean_dist = (centroids - global_mean).pow(2).sum(axis=1)\n",
    "    return (centroid_count*centroid_to_mean_dist).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYWjDmpkE7nn"
   },
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 18 clusters\n",
    "n_clusters = np.arange(2, 19)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 19\n",
    "for k in n_clusters:\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(pca_85_df)\n",
    "\n",
    "    # measure BCSS\n",
    "    \n",
    "    print(f'evaluating model with {k} clusters')\n",
    "    y_preds = km.predict(pca_85_df)\n",
    "    pca_85_df = pd.DataFrame(pca_85_df)\n",
    "    pca_85_df['cluster_label'] = y_preds\n",
    "    errors.append(between_cluster_variation(pca_85_df, 'cluster_label'))\n",
    "    print(errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37Mw4W0WFK2_"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Between-Cluster Sum of Squares (BCSS)')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors)\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZBFhE3yFWtc"
   },
   "source": [
    "### CH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDwpwU3xFZ0m"
   },
   "outputs": [],
   "source": [
    "def ch_index(df, label_col='label'):\n",
    "    n = len(df)\n",
    "    K = df[label_col].nunique()\n",
    "    B = between_cluster_variation(df, label_col)\n",
    "    W = within_cluster_variation(df, label_col)\n",
    "    return (B / (K-1)) / (W / (n-K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RakLe4AIFfpW"
   },
   "outputs": [],
   "source": [
    "# let's try everything between 2 and 18 clusters where 18 is the number of genres\n",
    "n_clusters = np.arange(2, 19)\n",
    "\n",
    "# store errors for each value of k\n",
    "errors = []\n",
    "\n",
    "# for i between 2 and 19\n",
    "for k in n_clusters:\n",
    "    print(f'training model with {k} clusters')\n",
    "    # perform k-means clustering\n",
    "    km = KMeans(n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    km.fit(pca_85_df)\n",
    "    \n",
    "    # measure CH\n",
    "    print(f'evaluating model with {k} clusters')\n",
    "    y_preds = km.predict(pca_85_df)\n",
    "    pca_85_df = pd.DataFrame(pca_85_df)\n",
    "    pca_85_df['cluster_label'] = y_preds\n",
    "    errors.append(ch_index(pca_85_df, 'cluster_label'))\n",
    "    print(errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLSSvGKUFkyG"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('CH index')\n",
    "plt.title('Elbow Method for Determining Optimal Value of k')\n",
    "plt.scatter(n_clusters, errors, color=\"#4DA017\")\n",
    "plt.plot(n_clusters, errors)\n",
    "plt.xticks(n_clusters)\n",
    "#plt.axvline(x=3, color='#4D17A0', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiumCSeaFtdD"
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "# Remember to set the random state for reproducibility\n",
    "km = KMeans(n_clusters=K, verbose=0, random_state=42)\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(pca_85_df)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58GSaz2jFysd"
   },
   "outputs": [],
   "source": [
    "# Obtain cluster memberships for each item in the data\n",
    "y_preds = km.predict(pca_85_df)\n",
    "pca_85_df['cluster_label'] = y_preds\n",
    "centers = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU_Ahpz1F2eO"
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "for k in range(K):\n",
    "    x1 = pca_85_df[pca_85_df['cluster_label'] == k][0]\n",
    "    x2 = pca_85_df[pca_85_df['cluster_label'] == k][1]\n",
    "    plt.scatter(x1, x2, label=\"k = \"+str(k+1),alpha=0.85)\n",
    "# Show cluster centroid locations    \n",
    "plt.scatter(centers[:,0],centers[:,1],label=\"centroid\")\n",
    "plt.legend()\n",
    "plt.title(f\"K = {K}\")\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IioZNcx2GE2E"
   },
   "source": [
    "### Training  model with a subset (100k samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PClvgYFNGJS9"
   },
   "outputs": [],
   "source": [
    "# Load the 100k dataset\n",
    "train.drop('timestamp', axis=1, inplace=True)\n",
    "train_subset = train[:100000]\n",
    "reader = Reader(rating_scale=(train_subset['rating'].min(), train_subset['rating'].max()))\n",
    "data = Dataset.load_from_df(train_subset[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0NH7vstGQrP"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 30, n_factors = 200, init_std_dev = 0.05, random_state=42)\n",
    "svd_test.fit(trainset)\n",
    "predictions = svd_test.test(testset)\n",
    "# Calculate RMSE\n",
    "svd_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pME5Qkw4GUCk"
   },
   "outputs": [],
   "source": [
    "np_test = NormalPredictor()\n",
    "np_test.fit(trainset)\n",
    "predictions = np_test.test(testset)\n",
    "# Calculate RMSE\n",
    "np_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYbVsNSqGX97"
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd','n_epochs': 40}\n",
    "blo_test = BaselineOnly(bsl_options=bsl_options)\n",
    "blo_test.fit(trainset)\n",
    "predictions = blo_test.test(testset)\n",
    "# Calculate RMSE\n",
    "blo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwPMwPyEGbu8"
   },
   "outputs": [],
   "source": [
    "nmf_test = NMF()\n",
    "nmf_test.fit(trainset)\n",
    "predictions = nmf_test.test(testset)\n",
    "# Calculate RMSE\n",
    "nmf_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVYXfOVNGf78"
   },
   "outputs": [],
   "source": [
    "slo_test = SlopeOne()\n",
    "slo_test.fit(trainset)\n",
    "predictions = slo_test.test(testset)\n",
    "# Calculate RMSE\n",
    "slo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJL7OSlOGjdy"
   },
   "outputs": [],
   "source": [
    "cc_test = CoClustering(random_state=42)\n",
    "cc_test.fit(trainset)\n",
    "predictions = cc_test.test(testset)\n",
    "# Calculate RMSE\n",
    "cc_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7iOlebRGoou"
   },
   "source": [
    "###  Content Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3EoYbkpGtSB"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(subset_size):\n",
    "    \"\"\"Prepare data for use within Content filtering algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subset_size : int\n",
    "        Number of movies to use within the algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas Dataframe\n",
    "        Subset of movies selected for content-based filtering.\n",
    "\n",
    "    \"\"\"\n",
    "    # Split genre data into individual words.\n",
    "    movies['keyWords'] = movies['genres'].str.replace('|', ' ')\n",
    "    # Subset of the data\n",
    "    movies_subset = movies[:subset_size]\n",
    "    return movies_subset\n",
    " \n",
    "def content_model(movie_list,top_n=10): \n",
    "    \"\"\"Performs Content filtering based upon a list of movies supplied\n",
    "       by the app user.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_list : list (str)\n",
    "        Favorite movies chosen by the app user.\n",
    "    top_n : type\n",
    "        Number of top recommendations to return to the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list (str)\n",
    "        Titles of the top-n movie recommendations to the user.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initializing the empty list of recommended movies\n",
    "    data = data_preprocessing(2000)\n",
    "    \n",
    "    # Instantiating and generating the count matrix\n",
    "    count_vec = CountVectorizer()\n",
    "    count_matrix = count_vec.fit_transform(data['keyWords'])\n",
    "    indices = pd.Series(data['title'])\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "    cosine_sim = pd.DataFrame(cosine_sim, index = data.index, columns = data.index)\n",
    "    \n",
    "    # Getting the index of the movie that matches the title\n",
    "    idx_1 = indices[indices == movie_list[0]].index[0]\n",
    "    idx_2 = indices[indices == movie_list[1]].index[0]\n",
    "    idx_3 = indices[indices == movie_list[2]].index[0]\n",
    "    \n",
    "    # Creating a Series with the similarity scores in descending order\n",
    "    rank_1 = cosine_sim[idx_1]\n",
    "    rank_2 = cosine_sim[idx_2]\n",
    "    rank_3 = cosine_sim[idx_3]\n",
    "    \n",
    "    # Calculating the scores\n",
    "    score_series_1 = pd.Series(rank_1).sort_values(ascending = False)\n",
    "    score_series_2 = pd.Series(rank_2).sort_values(ascending = False)\n",
    "    score_series_3 = pd.Series(rank_3).sort_values(ascending = False)\n",
    "    \n",
    "    # Getting the indexes of the 10 most similar movies\n",
    "    listings = score_series_1.append(score_series_2).append(score_series_3).sort_values(ascending = False)\n",
    "\n",
    "    # Store movie names\n",
    "    recommended_movies = []\n",
    "    \n",
    "    # Appending the names of movies\n",
    "    top_50_indexes = list(listings.iloc[1:50].index)\n",
    "    \n",
    "    # Removing chosen movies\n",
    "    top_indexes = np.setdiff1d(top_50_indexes,[idx_1,idx_2,idx_3])\n",
    "    for i in top_indexes[:top_n]:\n",
    "        recommended_movies.append(list(movies['title'])[i])\n",
    "    return recommended_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK8rsf_wG4h2"
   },
   "source": [
    "### Movies Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJA2j5EVGxq6"
   },
   "outputs": [],
   "source": [
    "movies = movies.dropna()\n",
    "movie_list = ['Grumpier Old Men (1995)','Ace Ventura: When Nature Calls (1995)','Father of the Bride Part II (1995)']\n",
    "content_model(movie_list,top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKb6LfDTHJmq"
   },
   "source": [
    "### Comparing *Models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FazhUiIXG2cK"
   },
   "outputs": [],
   "source": [
    "# Compare RMSE values between models\n",
    "fig,axis = plt.subplots(figsize=(8, 5))\n",
    "rmse_x = ['SVD','NormalPredictor','BaselineOnly','NMF','SlopeOne','CoClustering']\n",
    "rmse_y = [svd_rmse,np_rmse,blo_rmse,nmf_rmse,slo_rmse,cc_rmse]\n",
    "ax = sns.barplot(x=rmse_x, y=rmse_y,palette='brg',edgecolor='black')\n",
    "plt.title('RMSE Value Per Collaborative-based Filtering Model',fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('RMSE')\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),2), fontsize=12, ha=\"center\", va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l73wIbjXHSz4"
   },
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXfTZ5PMHX2s"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 40, n_factors = 200, init_std_dev = 0.05, random_state=42)\n",
    "# Run 5-fold cross-validation and print results\n",
    "a = cross_validate(svd_test, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YagirZ3yHj2q"
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd','n_epochs': 40}\n",
    "blo_test = BaselineOnly(bsl_options=bsl_options)\n",
    "# Run 5-fold cross-validation and print results\n",
    "b = cross_validate(blo_test, data, measures=['RMSE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GdaBE-uH1-l"
   },
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLqePm5gHzc5"
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_epochs':[40], #[30,40,50],\n",
    "              'n_factors':[400], #[100,200,300,400],\n",
    "              'init_std_dev':[0.005], #[0.001,0.005,0.05,0.1],\n",
    "              'random_state':[42]} \n",
    "grid_SVD = GridSearchCV(SVD, cv=5, measures=['rmse'], param_grid=param_grid, n_jobs=-1)\n",
    "grid_SVD.fit(data)\n",
    "print('***Best score:***')\n",
    "print(grid_SVD.best_score['rmse'])\n",
    "print('***Best parameters:***')\n",
    "print(grid_SVD.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJDyfrCEFWhz"
   },
   "source": [
    "### Use Best Parameters to Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc6d5pNdITX9"
   },
   "outputs": [],
   "source": [
    "svd_test = SVD(n_epochs = 40, n_factors = 400, init_std_dev = 0.005, random_state=42)\n",
    "svd_test.fit(trainset)\n",
    "predictions = svd_test.test(testset)\n",
    "# Calculate RMSE\n",
    "svd_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyLZ6nNBIXGb"
   },
   "outputs": [],
   "source": [
    "# Predicted Target Values vs. Actual Target Values\n",
    "new_df = pd.DataFrame(columns=['uid', 'iid', 'rating'])\n",
    "i = 0\n",
    "for (uid, iid, rating) in testset:\n",
    "    new_df.loc[i] = [uid, iid, rating]\n",
    "    i = i+1\n",
    "true = new_df['rating']\n",
    "pred = []\n",
    "for i in predictions:\n",
    "    pred.append(i.est)\n",
    "fig,axis = plt.subplots(figsize=(8, 5))\n",
    "sns.boxplot(x=true, y=pred, palette=\"brg\")\n",
    "plt.title(\"Predicted Target Values vs. Actual Target Values\", fontsize=14)\n",
    "plt.xlabel(\"Actual Target Values\")\n",
    "plt.ylabel(\"Predicted Target Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkTMZewgIe5d"
   },
   "source": [
    "### Training the whole Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE1ZCLFaInE_"
   },
   "outputs": [],
   "source": [
    "# Train model on whole dataset\n",
    "reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))\n",
    "data = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "svd = SVD(n_epochs = 30, n_factors = 300, init_std_dev = 0.005, random_state=42, verbose=True)\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Create Kaggle submission file\n",
    "predictions = []\n",
    "for i, row in test_df.iterrows():\n",
    "    x = (svd.predict(row.userId, row.movieId))\n",
    "    pred = x[3]\n",
    "    predictions.append(pred)\n",
    "test_df['Id'] = test_df['userId'].map(str) +'_'+ test_df['movieId'].map(str)\n",
    "results = pd.DataFrame({\"Id\":test_df['Id'],\"rating\": predictions})\n",
    "results.to_csv(\"ZF2_first_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5AcYqsu_1Qg"
   },
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RA9WmW3o_1Qh"
   },
   "outputs": [],
   "source": [
    "def content_generate_rating_estimate(movie_id, user, rating_data, k=20, threshold=0.0):\n",
    "    # Convert the movie title to a numeric index for our \n",
    "    # similarity matrix\n",
    "    b_idx = indices[movie_id]\n",
    "    pep = [] # <-- Stores our collection of similarity values \n",
    "     \n",
    "    # Gather the similarity ratings between each movie the user has rated\n",
    "    # and the reference movie \n",
    "    for index, row in rating_data[rating_data['userId']==user].iterrows():\n",
    "        sim = cosine_sim_tfidf[b_idx-1, indices[row['movieId']]-1]\n",
    "        pep.append((sim, row['rating']))\n",
    "    # Select the top-N values from our collection\n",
    "    k_pep = heapq.nlargest(k, pep, key=lambda t: t[0])\n",
    "\n",
    "    # Compute the weighted average using similarity scores and \n",
    "    # user item ratings. \n",
    "    simTotal, weightedSum = 0, 0\n",
    "    for (simScore, rating) in k_pep:\n",
    "        # Ensure that similarity ratings are above a given threshold\n",
    "        if (simScore > threshold):\n",
    "            simTotal += simScore\n",
    "            weightedSum += simScore * rating\n",
    "    try:\n",
    "        pred_rate = weightedSum / simTotal\n",
    "    except ZeroDivisionError:\n",
    "        # Cold-start problem - No ratings given by user. \n",
    "        # We use the average rating for the reference item as a proxy in this case \n",
    "        pred_rate = np.mean(rating_data[rating_data['movieId']==movie_id]['rating'])\n",
    "    return pred_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad0pvJYf_1Qi"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhGC8-rN_1Qj"
   },
   "outputs": [],
   "source": [
    "actual = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiWVeH7A_1Qk"
   },
   "outputs": [],
   "source": [
    "rmse = mean_squared_error((y_actual, pred_rate), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_BJQLI1_1Ql"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmhEH_VS_1Qm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eLqXblWDmJGr"
   ],
   "name": "Copy_of_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d462f8871f35b56244ff21fcecdd4cea7fae544baf36a688ae5371e728286c39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
